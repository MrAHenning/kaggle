{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from matplotlib import rcParams\n",
    "from itertools import chain\n",
    "from nltk import everygrams, word_tokenize\n",
    "import pprint\n",
    "import enchant\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents length 56370 \n",
      "                                       question_text                   qid\n",
      "0  My voice range is A2-C5. My chest voice goes u...  00014894849d00ba98a9\n",
      "1           How much does a tutor earn in Bangalore?  000156468431f09b3cae\n",
      "2  What are the best made pocket knives under $20...  000227734433360e1aae\n",
      "3  Why would they add a hypothetical scenario tha...  0005e06fbe3045bd2a92\n",
      "4   What is the dresscode for Techmahindra freshers?  00068a0f7f41f50fc399\n"
     ]
    }
   ],
   "source": [
    "#df_train = pd.read_csv('../kaggle_data_exploration/all/train.csv', encoding = \"utf_8\")\n",
    "df_test = pd.read_csv('../kaggle_data_exploration/all/test.csv', encoding = 'utf_8')\n",
    "data_t = df_test[['question_text']]\n",
    "data_t['qid'] = df_test.qid\n",
    "documents = data_t\n",
    "\n",
    "print('documents length {} \\n{}'.format(len(documents), documents[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatise and stemming functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a document to preview after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              question_text                   qid\n",
      "1  How much does a tutor earn in Bangalore?  000156468431f09b3cae\n",
      "original document: \n",
      "['How', 'much', 'does', 'a', 'tutor', 'earn', 'in', 'Bangalore?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['tutor', 'earn', 'bangalor']\n"
     ]
    }
   ],
   "source": [
    "#print(len(documents))\n",
    "#print(documents[1:2])\n",
    "doc_sample = documents[documents['qid']=='000156468431f09b3cae']\n",
    "print(doc_sample)\n",
    "doc_sample = doc_sample.values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the headline text, saving the results as ‘processed_docs’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['question_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [voic, rang, chest, voic, go, includ, sampl, h...\n",
       "1                              [tutor, earn, bangalor]\n",
       "2                                [best, pocket, knive]\n",
       "3    [hypothet, scenario, imposs, happen, link, sho...\n",
       "4                    [dresscod, techmahindra, fresher]\n",
       "5                                       [adapt, trump]\n",
       "6                                 [thing, peopl, life]\n",
       "7    [receiv, condit, offer, master, intern, busi, ...\n",
       "8                      [appareil, photo, mean, french]\n",
       "9                              [public, litig, canada]\n",
       "Name: question_text, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words on the Data set\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 chest\n",
      "1 go\n",
      "2 higher\n",
      "3 includ\n",
      "4 rang\n",
      "5 sampl\n",
      "6 type\n",
      "7 voic\n",
      "8 bangalor\n",
      "9 earn\n",
      "10 tutor\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out tokens that appear in less than 15 documents (absolute number) or more than 0.5 documents (fraction of total corpus size, not absolute number)\n",
    "\n",
    "\n",
    "Filter out tokens that appear in\n",
    "\n",
    "less than no_below documents (absolute number) or\n",
    "more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None).\n",
    "After the pruning, shrink resulting gaps in word ids.\n",
    "\n",
    "Note: Due to the gap shrinking, the same word may have a different word id before and after the call to this function!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim doc2bow\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 1), (9, 1), (10, 1)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview Bag Of Words for our sample preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 8 (\"bangalor\") appears 1 time.\n",
      "Word 9 (\"earn\") appears 1 time.\n",
      "Word 10 (\"tutor\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_1 = bow_corpus[1]\n",
    "for i in range(len(bow_doc_1)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1[i][0], \n",
    "                                               dictionary[bow_doc_1[i][0]], \n",
    "bow_doc_1[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.47464595144957933),\n",
      " (1, 0.15245407189012472),\n",
      " (2, 0.19815202840396937),\n",
      " (3, 0.196991567476024),\n",
      " (4, 0.43503012520630324),\n",
      " (5, 0.2416911528186504),\n",
      " (6, 0.16206519730077684),\n",
      " (7, 0.6320349051717128)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA using Bag of Words\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each topic, we will explore the words occuring in that topic and its relative weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.065*\"best\" + 0.019*\"learn\" + 0.015*\"human\" + 0.012*\"onlin\" + 0.012*\"north\" + 0.012*\"cours\" + 0.010*\"train\" + 0.009*\"electr\" + 0.009*\"develop\" + 0.008*\"join\"\n",
      "Topic: 1 \n",
      "Words: 0.045*\"mean\" + 0.026*\"work\" + 0.015*\"thing\" + 0.012*\"best\" + 0.012*\"possibl\" + 0.010*\"music\" + 0.010*\"say\" + 0.009*\"invest\" + 0.009*\"see\" + 0.009*\"bank\"\n",
      "Topic: 2 \n",
      "Words: 0.054*\"like\" + 0.025*\"feel\" + 0.023*\"good\" + 0.020*\"friend\" + 0.018*\"happen\" + 0.014*\"love\" + 0.013*\"girl\" + 0.013*\"better\" + 0.012*\"look\" + 0.012*\"form\"\n",
      "Topic: 3 \n",
      "Words: 0.050*\"peopl\" + 0.017*\"live\" + 0.012*\"feel\" + 0.012*\"anim\" + 0.011*\"american\" + 0.011*\"like\" + 0.010*\"english\" + 0.010*\"success\" + 0.009*\"chines\" + 0.009*\"high\"\n",
      "Topic: 4 \n",
      "Words: 0.035*\"quora\" + 0.025*\"know\" + 0.022*\"question\" + 0.014*\"answer\" + 0.012*\"play\" + 0.012*\"game\" + 0.012*\"websit\" + 0.011*\"time\" + 0.011*\"video\" + 0.011*\"peopl\"\n",
      "Topic: 5 \n",
      "Words: 0.016*\"prepar\" + 0.015*\"busi\" + 0.013*\"good\" + 0.013*\"main\" + 0.011*\"rank\" + 0.011*\"year\" + 0.011*\"presid\" + 0.011*\"exam\" + 0.011*\"start\" + 0.010*\"best\"\n",
      "Topic: 6 \n",
      "Words: 0.025*\"engin\" + 0.021*\"studi\" + 0.021*\"want\" + 0.015*\"year\" + 0.015*\"best\" + 0.013*\"colleg\" + 0.012*\"career\" + 0.009*\"work\" + 0.009*\"doctor\" + 0.008*\"program\"\n",
      "Topic: 7 \n",
      "Words: 0.035*\"indian\" + 0.017*\"write\" + 0.016*\"india\" + 0.013*\"import\" + 0.013*\"peopl\" + 0.011*\"white\" + 0.010*\"book\" + 0.010*\"muslim\" + 0.009*\"effect\" + 0.009*\"function\"\n",
      "Topic: 8 \n",
      "Words: 0.041*\"differ\" + 0.022*\"chang\" + 0.020*\"caus\" + 0.018*\"countri\" + 0.017*\"consid\" + 0.013*\"person\" + 0.011*\"social\" + 0.011*\"free\" + 0.010*\"state\" + 0.009*\"legal\"\n",
      "Topic: 9 \n",
      "Words: 0.041*\"think\" + 0.016*\"long\" + 0.016*\"trump\" + 0.015*\"year\" + 0.013*\"way\" + 0.011*\"type\" + 0.011*\"exampl\" + 0.011*\"world\" + 0.011*\"women\" + 0.009*\"start\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.008*\"effect\" + 0.007*\"best\" + 0.006*\"famous\" + 0.006*\"function\" + 0.006*\"year\" + 0.005*\"time\" + 0.005*\"power\" + 0.005*\"peopl\" + 0.005*\"today\" + 0.005*\"teacher\"\n",
      "Topic: 1 Word: 0.012*\"book\" + 0.010*\"best\" + 0.009*\"cours\" + 0.007*\"form\" + 0.007*\"travel\" + 0.007*\"hair\" + 0.006*\"happen\" + 0.006*\"number\" + 0.006*\"choos\" + 0.006*\"cost\"\n",
      "Topic: 2 Word: 0.008*\"black\" + 0.006*\"know\" + 0.006*\"peopl\" + 0.006*\"date\" + 0.006*\"affect\" + 0.005*\"think\" + 0.005*\"best\" + 0.005*\"world\" + 0.005*\"characterist\" + 0.005*\"india\"\n",
      "Topic: 3 Word: 0.009*\"india\" + 0.008*\"caus\" + 0.007*\"fight\" + 0.007*\"indian\" + 0.007*\"think\" + 0.006*\"peopl\" + 0.006*\"best\" + 0.006*\"china\" + 0.006*\"class\" + 0.005*\"experi\"\n",
      "Topic: 4 Word: 0.014*\"good\" + 0.011*\"life\" + 0.008*\"exampl\" + 0.007*\"colleg\" + 0.007*\"univers\" + 0.006*\"best\" + 0.006*\"role\" + 0.006*\"time\" + 0.006*\"women\" + 0.006*\"anim\"\n",
      "Topic: 5 Word: 0.019*\"best\" + 0.013*\"like\" + 0.012*\"work\" + 0.010*\"quora\" + 0.009*\"learn\" + 0.009*\"start\" + 0.009*\"question\" + 0.008*\"prepar\" + 0.007*\"live\" + 0.007*\"place\"\n",
      "Topic: 6 Word: 0.018*\"mean\" + 0.011*\"trump\" + 0.007*\"develop\" + 0.006*\"presid\" + 0.006*\"free\" + 0.006*\"major\" + 0.006*\"accomplish\" + 0.006*\"read\" + 0.005*\"valu\" + 0.005*\"donald\"\n",
      "Topic: 7 Word: 0.010*\"money\" + 0.009*\"better\" + 0.007*\"websit\" + 0.006*\"invest\" + 0.006*\"water\" + 0.006*\"best\" + 0.006*\"influenc\" + 0.006*\"happen\" + 0.006*\"creat\" + 0.005*\"hous\"\n",
      "Topic: 8 Word: 0.015*\"differ\" + 0.015*\"know\" + 0.012*\"peopl\" + 0.012*\"love\" + 0.010*\"thing\" + 0.010*\"feel\" + 0.009*\"rank\" + 0.008*\"like\" + 0.007*\"mark\" + 0.007*\"friend\"\n",
      "Topic: 9 Word: 0.010*\"countri\" + 0.008*\"peopl\" + 0.007*\"favorit\" + 0.006*\"stop\" + 0.006*\"muslim\" + 0.005*\"christian\" + 0.005*\"like\" + 0.005*\"song\" + 0.005*\"know\" + 0.005*\"think\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tutor', 'earn', 'bangalor']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.33928558230400085\t \n",
      "Topic: 0.065*\"best\" + 0.019*\"learn\" + 0.015*\"human\" + 0.012*\"onlin\" + 0.012*\"north\" + 0.012*\"cours\" + 0.010*\"train\" + 0.009*\"electr\" + 0.009*\"develop\" + 0.008*\"join\"\n",
      "\n",
      "Score: 0.26851847767829895\t \n",
      "Topic: 0.045*\"mean\" + 0.026*\"work\" + 0.015*\"thing\" + 0.012*\"best\" + 0.012*\"possibl\" + 0.010*\"music\" + 0.010*\"say\" + 0.009*\"invest\" + 0.009*\"see\" + 0.009*\"bank\"\n",
      "\n",
      "Score: 0.21719369292259216\t \n",
      "Topic: 0.025*\"engin\" + 0.021*\"studi\" + 0.021*\"want\" + 0.015*\"year\" + 0.015*\"best\" + 0.013*\"colleg\" + 0.012*\"career\" + 0.009*\"work\" + 0.009*\"doctor\" + 0.008*\"program\"\n",
      "\n",
      "Score: 0.025001665577292442\t \n",
      "Topic: 0.016*\"prepar\" + 0.015*\"busi\" + 0.013*\"good\" + 0.013*\"main\" + 0.011*\"rank\" + 0.011*\"year\" + 0.011*\"presid\" + 0.011*\"exam\" + 0.011*\"start\" + 0.010*\"best\"\n",
      "\n",
      "Score: 0.025000322610139847\t \n",
      "Topic: 0.041*\"differ\" + 0.022*\"chang\" + 0.020*\"caus\" + 0.018*\"countri\" + 0.017*\"consid\" + 0.013*\"person\" + 0.011*\"social\" + 0.011*\"free\" + 0.010*\"state\" + 0.009*\"legal\"\n",
      "\n",
      "Score: 0.025000248104333878\t \n",
      "Topic: 0.050*\"peopl\" + 0.017*\"live\" + 0.012*\"feel\" + 0.012*\"anim\" + 0.011*\"american\" + 0.011*\"like\" + 0.010*\"english\" + 0.010*\"success\" + 0.009*\"chines\" + 0.009*\"high\"\n",
      "\n",
      "Score: 0.02500000037252903\t \n",
      "Topic: 0.054*\"like\" + 0.025*\"feel\" + 0.023*\"good\" + 0.020*\"friend\" + 0.018*\"happen\" + 0.014*\"love\" + 0.013*\"girl\" + 0.013*\"better\" + 0.012*\"look\" + 0.012*\"form\"\n",
      "\n",
      "Score: 0.02500000037252903\t \n",
      "Topic: 0.035*\"quora\" + 0.025*\"know\" + 0.022*\"question\" + 0.014*\"answer\" + 0.012*\"play\" + 0.012*\"game\" + 0.012*\"websit\" + 0.011*\"time\" + 0.011*\"video\" + 0.011*\"peopl\"\n",
      "\n",
      "Score: 0.02500000037252903\t \n",
      "Topic: 0.035*\"indian\" + 0.017*\"write\" + 0.016*\"india\" + 0.013*\"import\" + 0.013*\"peopl\" + 0.011*\"white\" + 0.010*\"book\" + 0.010*\"muslim\" + 0.009*\"effect\" + 0.009*\"function\"\n",
      "\n",
      "Score: 0.02500000037252903\t \n",
      "Topic: 0.041*\"think\" + 0.016*\"long\" + 0.016*\"trump\" + 0.015*\"year\" + 0.013*\"way\" + 0.011*\"type\" + 0.011*\"exampl\" + 0.011*\"world\" + 0.011*\"women\" + 0.009*\"start\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6268593072891235\t \n",
      "Topic: 0.008*\"effect\" + 0.007*\"best\" + 0.006*\"famous\" + 0.006*\"function\" + 0.006*\"year\" + 0.005*\"time\" + 0.005*\"power\" + 0.005*\"peopl\" + 0.005*\"today\" + 0.005*\"teacher\"\n",
      "\n",
      "Score: 0.3115895092487335\t \n",
      "Topic: 0.008*\"black\" + 0.006*\"know\" + 0.006*\"peopl\" + 0.006*\"date\" + 0.006*\"affect\" + 0.005*\"think\" + 0.005*\"best\" + 0.005*\"world\" + 0.005*\"characterist\" + 0.005*\"india\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.42642053961753845\t Topic: 0.035*\"quora\" + 0.025*\"know\" + 0.022*\"question\" + 0.014*\"answer\" + 0.012*\"play\"\n",
      "Score: 0.41357311606407166\t Topic: 0.054*\"like\" + 0.025*\"feel\" + 0.023*\"good\" + 0.020*\"friend\" + 0.018*\"happen\"\n",
      "Score: 0.020003680139780045\t Topic: 0.016*\"prepar\" + 0.015*\"busi\" + 0.013*\"good\" + 0.013*\"main\" + 0.011*\"rank\"\n",
      "Score: 0.020001044496893883\t Topic: 0.041*\"differ\" + 0.022*\"chang\" + 0.020*\"caus\" + 0.018*\"countri\" + 0.017*\"consid\"\n",
      "Score: 0.020000869408249855\t Topic: 0.050*\"peopl\" + 0.017*\"live\" + 0.012*\"feel\" + 0.012*\"anim\" + 0.011*\"american\"\n",
      "Score: 0.020000237971544266\t Topic: 0.065*\"best\" + 0.019*\"learn\" + 0.015*\"human\" + 0.012*\"onlin\" + 0.012*\"north\"\n",
      "Score: 0.02000022865831852\t Topic: 0.041*\"think\" + 0.016*\"long\" + 0.016*\"trump\" + 0.015*\"year\" + 0.013*\"way\"\n",
      "Score: 0.02000017836689949\t Topic: 0.035*\"indian\" + 0.017*\"write\" + 0.016*\"india\" + 0.013*\"import\" + 0.013*\"peopl\"\n",
      "Score: 0.020000074058771133\t Topic: 0.045*\"mean\" + 0.026*\"work\" + 0.015*\"thing\" + 0.012*\"best\" + 0.012*\"possibl\"\n",
      "Score: 0.02000000886619091\t Topic: 0.025*\"engin\" + 0.021*\"studi\" + 0.021*\"want\" + 0.015*\"year\" + 0.015*\"best\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
