{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from matplotlib import rcParams\n",
    "from itertools import chain\n",
    "from nltk import everygrams, word_tokenize\n",
    "import pprint\n",
    "import enchant\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the datatset from which we will extract the BoW for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../kaggle_data_exploration/all/train.csv', encoding = \"utf_8\")\n",
    "#df_test = pd.read_csv('../kaggle_data_exploration/all/test.csv', encoding = 'utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    qid                                      question_text  \\\n",
      "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
      "\n",
      "   target  \n",
      "0       0  \n"
     ]
    }
   ],
   "source": [
    "pprint(df_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents length 1306122 \n",
      "                                       question_text                   qid\n",
      "0  How did Quebec nationalists see their province...  00002165364db923c7e6\n",
      "1  Do you have an adopted dog, how would you enco...  000032939017120e6e44\n",
      "2  Why does velocity affect time? Does velocity a...  0000412ca6e4628ce2cf\n",
      "3  How did Otto von Guericke used the Magdeburg h...  000042bf85aa498cd78e\n",
      "4  Can I convert montra helicon D to a mountain b...  0000455dfa3e01eae3af\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#df_train = pd.read_csv('../kaggle_data_exploration/all/train.csv', encoding = \"utf_8\")\n",
    "#df_train = pd.read_csv('../kaggle_data_exploration/all/train.csv', encoding = 'utf_8')\n",
    "data_t = df_train[['question_text']]\n",
    "data_t['qid'] = df_train.qid\n",
    "documents = data_t\n",
    "\n",
    "print('documents length {} \\n{}'.format(len(documents), documents[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Document after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       question_text                   qid\n",
      "0  How did Quebec nationalists see their province...  00002165364db923c7e6\n",
      "\n",
      "\n",
      "question_text    How did Quebec nationalists see their province...\n",
      "qid                                           00002165364db923c7e6\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['qid']=='00002165364db923c7e6']\n",
    "print(doc_sample)\n",
    "print('\\n')\n",
    "doc_sample2 = documents.loc[0]\n",
    "print(doc_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['How', 'did', 'Quebec', 'nationalists', 'see', 'their', 'province', 'as', 'a', 'nation', 'in', 'the', '1960s?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['quebec', 'nationalist', 'provinc', 'nation']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = doc_sample.values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How did Quebec nationalists see their province as a nation in the 1960s?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess the whole set\n",
    "documents['question_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                [adopt, encourag, peopl, adopt, shop]\n",
       "2    [veloc, affect, time, veloc, affect, space, ge...\n",
       "3                [otto, guerick, magdeburg, hemispher]\n",
       "4    [convert, montra, helicon, mountain, bike, cha...\n",
       "5    [gaza, slowli, auschwitz, dachau, treblinka, p...\n",
       "6    [quora, automat, conserv, opinion, report, lib...\n",
       "7                   [crazi, wash, wipe, groceri, germ]\n",
       "8         [thing, dress, moder, differ, dress, modest]\n",
       "9    [phase, ignor, peopl, love, complet, disregard...\n",
       "Name: question_text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['question_text'].map(preprocess)\n",
    "processed_docs[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306122\n",
      "0 nation\n",
      "1 nationalist\n",
      "2 provinc\n",
      "3 quebec\n",
      "4 adopt\n",
      "5 encourag\n",
      "6 peopl\n",
      "7 shop\n",
      "8 affect\n",
      "9 geometri\n",
      "10 space\n",
      "11 time\n",
      "12 veloc\n",
      "13 guerick\n",
      "14 hemispher\n",
      "15 magdeburg\n",
      "16 otto\n",
      "17 bike\n",
      "18 chang\n",
      "19 convert\n",
      "20 helicon\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_docs))\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 2), (5, 1), (6, 1), (7, 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 4 (\"adopt\") appears 2 time.\n",
      "Word 5 (\"encourag\") appears 1 time.\n",
      "Word 6 (\"peopl\") appears 1 time.\n",
      "Word 7 (\"shop\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_1 = bow_corpus[1]\n",
    "for i in range(len(bow_doc_1)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1[i][0], \n",
    "                                               dictionary[bow_doc_1[i][0]], \n",
    "bow_doc_1[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.074*\"best\" + 0.018*\"import\" + 0.017*\"place\" + 0.013*\"way\" + 0.012*\"good\" + 0.012*\"term\" + 0.012*\"travel\" + 0.012*\"build\" + 0.011*\"food\" + 0.010*\"increas\"\n",
      "Topic: 1 \n",
      "Words: 0.019*\"caus\" + 0.015*\"happen\" + 0.015*\"social\" + 0.013*\"parent\" + 0.013*\"water\" + 0.013*\"stop\" + 0.013*\"effect\" + 0.012*\"lose\" + 0.010*\"bodi\" + 0.009*\"media\"\n",
      "Topic: 2 \n",
      "Words: 0.027*\"quora\" + 0.023*\"question\" + 0.023*\"write\" + 0.023*\"book\" + 0.016*\"movi\" + 0.015*\"answer\" + 0.014*\"read\" + 0.014*\"game\" + 0.013*\"play\" + 0.010*\"follow\"\n",
      "Topic: 3 \n",
      "Words: 0.022*\"compani\" + 0.018*\"start\" + 0.018*\"money\" + 0.015*\"busi\" + 0.015*\"onlin\" + 0.014*\"market\" + 0.013*\"number\" + 0.013*\"account\" + 0.012*\"product\" + 0.010*\"india\"\n",
      "Topic: 4 \n",
      "Words: 0.023*\"friend\" + 0.023*\"love\" + 0.019*\"human\" + 0.016*\"languag\" + 0.015*\"have\" + 0.013*\"year\" + 0.012*\"like\" + 0.012*\"relationship\" + 0.012*\"state\" + 0.011*\"tell\"\n",
      "Topic: 5 \n",
      "Words: 0.026*\"work\" + 0.013*\"problem\" + 0.010*\"hous\" + 0.010*\"face\" + 0.010*\"time\" + 0.008*\"function\" + 0.008*\"phone\" + 0.008*\"project\" + 0.007*\"drive\" + 0.007*\"space\"\n",
      "Topic: 6 \n",
      "Words: 0.023*\"engin\" + 0.021*\"univers\" + 0.020*\"student\" + 0.019*\"year\" + 0.019*\"studi\" + 0.019*\"colleg\" + 0.018*\"best\" + 0.018*\"school\" + 0.017*\"good\" + 0.012*\"class\"\n",
      "Topic: 7 \n",
      "Words: 0.060*\"differ\" + 0.040*\"know\" + 0.036*\"learn\" + 0.021*\"girl\" + 0.018*\"type\" + 0.015*\"major\" + 0.011*\"watch\" + 0.011*\"develop\" + 0.011*\"wear\" + 0.010*\"like\"\n",
      "Topic: 8 \n",
      "Words: 0.054*\"peopl\" + 0.033*\"like\" + 0.029*\"person\" + 0.029*\"life\" + 0.025*\"mean\" + 0.018*\"feel\" + 0.016*\"american\" + 0.016*\"women\" + 0.016*\"look\" + 0.014*\"want\"\n",
      "Topic: 9 \n",
      "Words: 0.024*\"thing\" + 0.022*\"world\" + 0.022*\"countri\" + 0.020*\"india\" + 0.016*\"think\" + 0.016*\"trump\" + 0.015*\"peopl\" + 0.015*\"go\" + 0.012*\"indian\" + 0.011*\"muslim\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying the model with some instances..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adopt', 'encourag', 'peopl', 'adopt', 'shop']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5041003823280334\t \n",
      "Topic: 0.054*\"peopl\" + 0.033*\"like\" + 0.029*\"person\" + 0.029*\"life\" + 0.025*\"mean\" + 0.018*\"feel\" + 0.016*\"american\" + 0.016*\"women\" + 0.016*\"look\" + 0.014*\"want\"\n",
      "\n",
      "Score: 0.19589513540267944\t \n",
      "Topic: 0.024*\"thing\" + 0.022*\"world\" + 0.022*\"countri\" + 0.020*\"india\" + 0.016*\"think\" + 0.016*\"trump\" + 0.015*\"peopl\" + 0.015*\"go\" + 0.012*\"indian\" + 0.011*\"muslim\"\n",
      "\n",
      "Score: 0.18333394825458527\t \n",
      "Topic: 0.022*\"compani\" + 0.018*\"start\" + 0.018*\"money\" + 0.015*\"busi\" + 0.015*\"onlin\" + 0.014*\"market\" + 0.013*\"number\" + 0.013*\"account\" + 0.012*\"product\" + 0.010*\"india\"\n",
      "\n",
      "Score: 0.01667051762342453\t \n",
      "Topic: 0.074*\"best\" + 0.018*\"import\" + 0.017*\"place\" + 0.013*\"way\" + 0.012*\"good\" + 0.012*\"term\" + 0.012*\"travel\" + 0.012*\"build\" + 0.011*\"food\" + 0.010*\"increas\"\n",
      "\n",
      "Score: 0.01666666753590107\t \n",
      "Topic: 0.019*\"caus\" + 0.015*\"happen\" + 0.015*\"social\" + 0.013*\"parent\" + 0.013*\"water\" + 0.013*\"stop\" + 0.013*\"effect\" + 0.012*\"lose\" + 0.010*\"bodi\" + 0.009*\"media\"\n",
      "\n",
      "Score: 0.01666666753590107\t \n",
      "Topic: 0.027*\"quora\" + 0.023*\"question\" + 0.023*\"write\" + 0.023*\"book\" + 0.016*\"movi\" + 0.015*\"answer\" + 0.014*\"read\" + 0.014*\"game\" + 0.013*\"play\" + 0.010*\"follow\"\n",
      "\n",
      "Score: 0.01666666753590107\t \n",
      "Topic: 0.023*\"friend\" + 0.023*\"love\" + 0.019*\"human\" + 0.016*\"languag\" + 0.015*\"have\" + 0.013*\"year\" + 0.012*\"like\" + 0.012*\"relationship\" + 0.012*\"state\" + 0.011*\"tell\"\n",
      "\n",
      "Score: 0.01666666753590107\t \n",
      "Topic: 0.026*\"work\" + 0.013*\"problem\" + 0.010*\"hous\" + 0.010*\"face\" + 0.010*\"time\" + 0.008*\"function\" + 0.008*\"phone\" + 0.008*\"project\" + 0.007*\"drive\" + 0.007*\"space\"\n",
      "\n",
      "Score: 0.01666666753590107\t \n",
      "Topic: 0.023*\"engin\" + 0.021*\"univers\" + 0.020*\"student\" + 0.019*\"year\" + 0.019*\"studi\" + 0.019*\"colleg\" + 0.018*\"best\" + 0.018*\"school\" + 0.017*\"good\" + 0.012*\"class\"\n",
      "\n",
      "Score: 0.01666666753590107\t \n",
      "Topic: 0.060*\"differ\" + 0.040*\"know\" + 0.036*\"learn\" + 0.021*\"girl\" + 0.018*\"type\" + 0.015*\"major\" + 0.011*\"watch\" + 0.011*\"develop\" + 0.011*\"wear\" + 0.010*\"like\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying the model with all training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "\n",
    "def averageTL(tuplelist):\n",
    "    d = {}\n",
    "    for el in tuplelist:\n",
    "        for tuple in el:\n",
    "            key,val = tuple\n",
    "            d.setdefault(key, []).append(val)\n",
    "     \n",
    "    return(d) \n",
    "    \n",
    "    \n",
    "    \n",
    "#mec = [[(0, 0.016670508), (1, 0.016666668), (2, 0.016666668), (3, 0.18333398), (4, 0.016666668), (5, 0.016666668), (6, 0.016666668), (7, 0.016666668), (8, 0.50409806), (9, 0.19589746)], [(0, 0.012500407), (1, 0.28031057), (2, 0.012500324), (3, 0.012500497), (4, 0.23546916), (5, 0.26042563), (6, 0.14879261), (7, 0.012500024), (8, 0.012500535), (9, 0.012500283)], [(0, 0.7), (1, 0.033333335), (2, 0.033333335), (3, 0.033333335), (4, 0.033333335), (5, 0.033333335), (6, 0.033333335), (7, 0.033333335), (8, 0.033333335), (9, 0.033333335)], [(0, 0.016666668), (1, 0.016679456), (2, 0.016668567), (3, 0.51121813), (4, 0.016668288), (5, 0.016671726), (6, 0.016667815), (7, 0.01666667), (8, 0.016670717), (9, 0.35542193)], [(0, 0.025), (1, 0.025), (2, 0.025001911), (3, 0.025), (4, 0.025), (5, 0.275), (6, 0.025), (7, 0.025), (8, 0.5249981), (9, 0.025)], [(0, 0.0125), (1, 0.0125), (2, 0.28289184), (3, 0.012503062), (4, 0.0125), (5, 0.13749853), (6, 0.0125), (7, 0.012501531), (8, 0.0125003345), (9, 0.4921047)], [(0, 0.016666668), (1, 0.183333), (2, 0.016666668), (3, 0.016666668), (4, 0.016666668), (5, 0.18333335), (6, 0.016666668), (7, 0.35000035), (8, 0.016666668), (9, 0.1833333)], [(0, 0.014285714), (1, 0.014285714), (2, 0.014285714), (3, 0.44285735), (4, 0.014285714), (5, 0.014285714), (6, 0.1571425), (7, 0.15714295), (8, 0.014285714), (9, 0.15714289)], [(3, 0.15684174), (4, 0.08903002), (7, 0.07857286), (8, 0.557643), (9, 0.08219377)]]\n",
    "#averageTL(mec,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDfAndAverage(df, LDAModel):\n",
    "    texts = df['question_text']\n",
    "    print('len {}\\n{}'.format(len(texts), texts[:5]))\n",
    "    preprocessed_texts = [preprocess(text) for text in texts]\n",
    "    print('len {}\\n{}'.format(len(preprocessed_texts), preprocessed_texts[:5]))\n",
    "    bow_vectors = [dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "    print('len {}\\n{}'.format(len(bow_vectors), bow_vectors[:5]))\n",
    "    lda_results = [lda_model[bow] for bow in bow_vectors]\n",
    "    print('len {}\\n{}'.format(len(lda_results), lda_results[:10]))\n",
    "    print('RESULTS')\n",
    "    \n",
    "    atl = averageTL(lda_results)\n",
    "    for name, values in atl.items():\n",
    "        print(\"{name} {avg}\".format(name=name, avg=sum(values)/len(lda_results)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "len 100000\n",
      "0    How did Quebec nationalists see their province...\n",
      "1    Do you have an adopted dog, how would you enco...\n",
      "2    Why does velocity affect time? Does velocity a...\n",
      "3    How did Otto von Guericke used the Magdeburg h...\n",
      "4    Can I convert montra helicon D to a mountain b...\n",
      "Name: question_text, dtype: object\n",
      "len 100000\n",
      "[['quebec', 'nationalist', 'provinc', 'nation'], ['adopt', 'encourag', 'peopl', 'adopt', 'shop'], ['veloc', 'affect', 'time', 'veloc', 'affect', 'space', 'geometri'], ['otto', 'guerick', 'magdeburg', 'hemispher'], ['convert', 'montra', 'helicon', 'mountain', 'bike', 'chang', 'tyre']]\n",
      "len 100000\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 2), (5, 1), (6, 1), (7, 1)], [(8, 2), (9, 1), (10, 1), (11, 1), (12, 2)], [(13, 1), (14, 1)], [(15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]]\n",
      "len 100000\n",
      "[[(0, 0.02), (1, 0.02), (2, 0.02), (3, 0.02), (4, 0.41997617), (5, 0.21999998), (6, 0.02), (7, 0.22000027), (8, 0.02), (9, 0.020023575)], [(0, 0.016670514), (1, 0.016666668), (2, 0.016666668), (3, 0.18333398), (4, 0.016666668), (5, 0.016666668), (6, 0.016666668), (7, 0.016666668), (8, 0.50409883), (9, 0.19589667)], [(0, 0.012500408), (1, 0.28037035), (2, 0.012500324), (3, 0.012500498), (4, 0.23727563), (5, 0.25850657), (6, 0.14884537), (7, 0.012500024), (8, 0.012500537), (9, 0.012500284)], [(0, 0.7), (1, 0.033333335), (2, 0.033333335), (3, 0.033333335), (4, 0.033333335), (5, 0.033333335), (6, 0.033333335), (7, 0.033333335), (8, 0.033333335), (9, 0.033333335)], [(0, 0.016666668), (1, 0.016676346), (2, 0.016668564), (3, 0.5112291), (4, 0.016668284), (5, 0.016671719), (6, 0.016667813), (7, 0.016666668), (8, 0.016670711), (9, 0.35541406)], [(0, 0.025), (1, 0.025), (2, 0.025001913), (3, 0.025), (4, 0.025), (5, 0.275), (6, 0.025), (7, 0.025), (8, 0.5249981), (9, 0.025)], [(0, 0.012500001), (1, 0.012500001), (2, 0.28293994), (3, 0.012503063), (4, 0.012500001), (5, 0.13749853), (6, 0.012500001), (7, 0.012501532), (8, 0.012500335), (9, 0.49205664)], [(0, 0.016666668), (1, 0.183333), (2, 0.016666668), (3, 0.016666668), (4, 0.016666668), (5, 0.18333335), (6, 0.016666668), (7, 0.35000038), (8, 0.016666668), (9, 0.1833333)], [(0, 0.014285715), (1, 0.014285715), (2, 0.014285715), (3, 0.44285738), (4, 0.014285715), (5, 0.014285715), (6, 0.15714249), (7, 0.15714295), (8, 0.014285715), (9, 0.15714289)], [(3, 0.15684175), (4, 0.089029655), (7, 0.07857286), (8, 0.55764335), (9, 0.08219379)]]\n",
      "RESULTS\n",
      "0 0.09626228775804863\n",
      "1 0.09197478927380405\n",
      "2 0.09636376961664296\n",
      "3 0.10615349909642711\n",
      "4 0.09403626984717324\n",
      "5 0.0935978446182143\n",
      "6 0.12034286599889397\n",
      "7 0.08224596025477164\n",
      "8 0.11159636819471605\n",
      "9 0.10434875216570683\n"
     ]
    }
   ],
   "source": [
    "sub_0 = df_train.loc[df_train['target']==0][:100000]\n",
    "print(len(sub_0))\n",
    "processDfAndAverage(sub_0, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80810\n",
      "len 80810\n",
      "22     Has the United States become the largest dicta...\n",
      "30     Which babies are more sweeter to their parents...\n",
      "110    If blacks support school choice and mandatory ...\n",
      "114    I am gay boy and I love my cousin (boy). He is...\n",
      "115                 Which races have the smallest penis?\n",
      "Name: question_text, dtype: object\n",
      "len 80810\n",
      "[['unit', 'state', 'largest', 'dictatorship', 'world'], ['babi', 'sweeter', 'parent', 'dark', 'skin', 'babi', 'light', 'skin', 'babi'], ['black', 'support', 'school', 'choic', 'mandatori', 'sentenc', 'crimin', 'vote', 'republican'], ['love', 'cousin', 'sexi', 'dont', 'know', 'want'], ['race', 'smallest', 'peni']]\n",
      "len 80810\n",
      "[[(96, 1), (97, 1), (98, 1), (99, 1), (100, 1)], [(77, 2), (131, 3), (132, 1), (133, 1), (134, 1)], [(135, 1), (175, 1), (432, 1), (433, 1), (434, 1), (435, 1), (436, 1), (437, 1), (438, 1)], [(46, 1), (68, 1), (449, 1), (450, 1), (451, 1), (452, 1)], [(453, 1), (454, 1), (455, 1)]]\n",
      "len 80810\n",
      "[[(0, 0.016666668), (1, 0.016666668), (2, 0.18333322), (3, 0.016666668), (4, 0.31204578), (5, 0.016666668), (6, 0.016667044), (7, 0.18333335), (8, 0.016666668), (9, 0.2212873)], [(0, 0.011111111), (1, 0.5666667), (2, 0.011111111), (3, 0.011111111), (4, 0.011111111), (5, 0.011111111), (6, 0.011111111), (7, 0.011111111), (8, 0.34444442), (9, 0.011111111)], [(0, 0.010002961), (6, 0.28509864), (8, 0.2348988), (9, 0.40999955)], [(0, 0.014285714), (1, 0.15714279), (2, 0.014285735), (3, 0.014287238), (4, 0.15862535), (5, 0.014285714), (6, 0.014287265), (7, 0.2826821), (8, 0.014290795), (9, 0.31582734)], [(0, 0.024999997), (1, 0.024999997), (2, 0.024999997), (3, 0.025000101), (4, 0.275), (5, 0.27499983), (6, 0.024999997), (7, 0.27500004), (8, 0.024999997), (9, 0.024999997)], [(0, 0.025), (1, 0.025), (2, 0.025), (3, 0.025), (4, 0.27500004), (5, 0.025), (6, 0.025), (7, 0.025), (8, 0.525), (9, 0.025)], [(0, 0.014285714), (1, 0.15714285), (2, 0.014285714), (3, 0.014285714), (4, 0.014290956), (5, 0.15714286), (6, 0.014285714), (7, 0.014285714), (8, 0.44285193), (9, 0.15714285)], [(2, 0.09166637), (4, 0.26709887), (7, 0.48666665), (8, 0.104556404)], [(0, 0.014285714), (1, 0.15714267), (2, 0.014285714), (3, 0.014286535), (4, 0.014285714), (5, 0.014285714), (6, 0.29999995), (7, 0.014285714), (8, 0.0142892245), (9, 0.442853)], [(0, 0.08461539), (1, 0.10216963), (3, 0.091074534), (4, 0.5288068), (9, 0.15486915)]]\n",
      "RESULTS\n",
      "0 0.054219029045319986\n",
      "1 0.08547894668276367\n",
      "2 0.07914237458888815\n",
      "3 0.05560034369049939\n",
      "4 0.09852805337957948\n",
      "5 0.05197187421671459\n",
      "6 0.04789510507518286\n",
      "7 0.08835467332324075\n",
      "8 0.203252112139913\n",
      "9 0.22531739192785571\n"
     ]
    }
   ],
   "source": [
    "sub_1 = df_train.loc[df_train['target']==1]\n",
    "print(len(sub_1))\n",
    "processDfAndAverage(sub_1, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.074*\"best\" + 0.018*\"import\" + 0.017*\"place\" + 0.013*\"way\" + 0.012*\"good\" + 0.012*\"term\" + 0.012*\"travel\" + 0.012*\"build\" + 0.011*\"food\" + 0.010*\"increas\"'),\n",
       " (1,\n",
       "  '0.019*\"caus\" + 0.015*\"happen\" + 0.015*\"social\" + 0.013*\"parent\" + 0.013*\"water\" + 0.013*\"stop\" + 0.013*\"effect\" + 0.012*\"lose\" + 0.010*\"bodi\" + 0.009*\"media\"'),\n",
       " (2,\n",
       "  '0.027*\"quora\" + 0.023*\"question\" + 0.023*\"write\" + 0.023*\"book\" + 0.016*\"movi\" + 0.015*\"answer\" + 0.014*\"read\" + 0.014*\"game\" + 0.013*\"play\" + 0.010*\"follow\"'),\n",
       " (3,\n",
       "  '0.022*\"compani\" + 0.018*\"start\" + 0.018*\"money\" + 0.015*\"busi\" + 0.015*\"onlin\" + 0.014*\"market\" + 0.013*\"number\" + 0.013*\"account\" + 0.012*\"product\" + 0.010*\"india\"'),\n",
       " (4,\n",
       "  '0.023*\"friend\" + 0.023*\"love\" + 0.019*\"human\" + 0.016*\"languag\" + 0.015*\"have\" + 0.013*\"year\" + 0.012*\"like\" + 0.012*\"relationship\" + 0.012*\"state\" + 0.011*\"tell\"'),\n",
       " (5,\n",
       "  '0.026*\"work\" + 0.013*\"problem\" + 0.010*\"hous\" + 0.010*\"face\" + 0.010*\"time\" + 0.008*\"function\" + 0.008*\"phone\" + 0.008*\"project\" + 0.007*\"drive\" + 0.007*\"space\"'),\n",
       " (6,\n",
       "  '0.023*\"engin\" + 0.021*\"univers\" + 0.020*\"student\" + 0.019*\"year\" + 0.019*\"studi\" + 0.019*\"colleg\" + 0.018*\"best\" + 0.018*\"school\" + 0.017*\"good\" + 0.012*\"class\"'),\n",
       " (7,\n",
       "  '0.060*\"differ\" + 0.040*\"know\" + 0.036*\"learn\" + 0.021*\"girl\" + 0.018*\"type\" + 0.015*\"major\" + 0.011*\"watch\" + 0.011*\"develop\" + 0.011*\"wear\" + 0.010*\"like\"'),\n",
       " (8,\n",
       "  '0.054*\"peopl\" + 0.033*\"like\" + 0.029*\"person\" + 0.029*\"life\" + 0.025*\"mean\" + 0.018*\"feel\" + 0.016*\"american\" + 0.016*\"women\" + 0.016*\"look\" + 0.014*\"want\"'),\n",
       " (9,\n",
       "  '0.024*\"thing\" + 0.022*\"world\" + 0.022*\"countri\" + 0.020*\"india\" + 0.016*\"think\" + 0.016*\"trump\" + 0.015*\"peopl\" + 0.015*\"go\" + 0.012*\"indian\" + 0.011*\"muslim\"')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a new row and saving the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    qid                                      question_text  \\\n",
      "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
      "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
      "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
      "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
      "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
      "\n",
      "   target         e  \n",
      "0       0  1.389152  \n",
      "1       0  1.326635  \n",
      "2       0  2.257668  \n",
      "3       0 -0.029960  \n",
      "4       0  0.413381  \n",
      "                          qid  \\\n",
      "1306117  ffffcc4e2331aaf1e41e   \n",
      "1306118  ffffd431801e5a2f4861   \n",
      "1306119  ffffd48fb36b63db010c   \n",
      "1306120  ffffec519fa37cf60c78   \n",
      "1306121  ffffed09fedb5088744a   \n",
      "\n",
      "                                             question_text  target         e  \n",
      "1306117  What other technical skills do you need as a c...       0  0.524521  \n",
      "1306118  Does MS in ECE have good job prospects in USA ...       0  0.273809  \n",
      "1306119                          Is foam insulation toxic?       0 -0.796287  \n",
      "1306120  How can one start a research project based on ...       0  0.201976  \n",
      "1306121  Who wins in a battle between a Wolverine and a...       0  1.261134  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1 = df_train\n",
    "df1 = df1.assign(e=pd.Series(np.random.randn(len(df1))).values)\n",
    "print(df1.head())\n",
    "print(df1.tail())\n",
    "df1.to_csv('test.csv', sep='\\t', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3499702513217926\t Topic: 0.049*\"like\" + 0.030*\"peopl\" + 0.025*\"feel\" + 0.024*\"know\" + 0.023*\"person\"\n",
      "Score: 0.18336229026317596\t Topic: 0.034*\"countri\" + 0.030*\"world\" + 0.023*\"state\" + 0.015*\"american\" + 0.014*\"peopl\"\n",
      "Score: 0.1833333969116211\t Topic: 0.027*\"learn\" + 0.018*\"develop\" + 0.017*\"mean\" + 0.014*\"compani\" + 0.012*\"product\"\n",
      "Score: 0.1833331435918808\t Topic: 0.080*\"best\" + 0.018*\"india\" + 0.015*\"effect\" + 0.013*\"place\" + 0.012*\"water\"\n",
      "Score: 0.016667576506733894\t Topic: 0.029*\"think\" + 0.023*\"peopl\" + 0.020*\"trump\" + 0.019*\"indian\" + 0.014*\"right\"\n",
      "Score: 0.01666666753590107\t Topic: 0.033*\"work\" + 0.026*\"engin\" + 0.018*\"student\" + 0.017*\"colleg\" + 0.014*\"studi\"\n",
      "Score: 0.01666666753590107\t Topic: 0.043*\"good\" + 0.031*\"book\" + 0.025*\"best\" + 0.022*\"money\" + 0.020*\"write\"\n",
      "Score: 0.01666666753590107\t Topic: 0.026*\"quora\" + 0.022*\"life\" + 0.022*\"question\" + 0.019*\"chang\" + 0.015*\"answer\"\n",
      "Score: 0.01666666753590107\t Topic: 0.018*\"import\" + 0.015*\"class\" + 0.015*\"univers\" + 0.014*\"exam\" + 0.012*\"prepar\"\n",
      "Score: 0.01666666753590107\t Topic: 0.020*\"year\" + 0.014*\"invest\" + 0.013*\"cost\" + 0.012*\"hous\" + 0.011*\"black\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
